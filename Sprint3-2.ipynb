{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04e30b84-af16-4e9a-b703-76658e4beae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.6\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f0b3dc1-1ce4-4d50-a9e3-53e0e61b4854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b5ca37e-da59-4b69-be73-2fcabe2c1c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "conf = (SparkConf()\n",
    "            .setMaster(\"yarn\")\n",
    "            .set(\"spark.executor.cores\", 5)\n",
    "            .set(\"spark.sql.shuffle.partitions\", 200)\n",
    "            .set(\"spark.default.parallelism\", 200)\n",
    "            .set(\"spark.executor.memory\", \"7g\")\n",
    "            .set(\"spark.dynamicAllocation.maxExecutors\", 20)\n",
    "        )\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .appName(\"doge\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b9d14d0-a56b-40f5-832f-4e69427ae562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of the loaded data:\n",
      "root\n",
      " |-- datetime: timestamp (nullable = true)\n",
      " |-- open: double (nullable = true)\n",
      " |-- high: double (nullable = true)\n",
      " |-- low: double (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      " |-- volume: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      "\n",
      "Number of rows: 6576\n",
      "Number of columns: 8\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de creaciÃ³n de un DataFrame a partir de los datos almacenados en HDFS\n",
    "df = spark.read \\\n",
    "          .option(\"header\",\"true\") \\\n",
    "          .option(\"inferSchema\", \"true\") \\\n",
    "          .csv(\"/datos/gittba26/gittba02/DOGE/doge_bronze\")\n",
    "\n",
    "print(f\"Schema of the loaded data:\")\n",
    "df.printSchema()\n",
    "print(f\"Number of rows: {df.count()}\")\n",
    "print(f\"Number of columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e36132f9-bc10-4812-917f-41727f797fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data successfully converted to Parquet format at '/datos/gittba26/gittba02/DOGE/doge_silver/'\n"
     ]
    }
   ],
   "source": [
    "# Define output path for parquet\n",
    "parquet_output = \"/datos/gittba26/gittba02/DOGE/doge_silver\"\n",
    "\n",
    "# Write the DataFrame as Parquet format\n",
    "df.write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .partitionBy(\"year\", \"month\") \\\n",
    "  .parquet(parquet_output)\n",
    "\n",
    "print(f\"\\nData successfully converted to Parquet format at '{parquet_output}/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1067c7a5-4381-4c9c-a2bb-af42801c3c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet verification - Rows: 6576, Columns: 8\n",
      "+-------------------+------+------+------+------+-----------+----+-----+\n",
      "|           datetime|  open|  high|   low| close|     volume|year|month|\n",
      "+-------------------+------+------+------+------+-----------+----+-----+\n",
      "|2025-08-01 02:00:00|0.2095|0.2113|0.2026|0.2097|1.2966565E7|2025|    8|\n",
      "|2025-08-01 06:00:00|0.2082|0.2087|0.2038| 0.206|  5124193.0|2025|    8|\n",
      "|2025-08-01 10:00:00|0.2059|0.2076|0.2005|0.2076|  6198772.0|2025|    8|\n",
      "|2025-08-01 14:00:00|0.2075|0.2105|0.2022|  0.21|  3038858.0|2025|    8|\n",
      "|2025-08-01 18:00:00|  0.21|0.2129|0.2039|0.2084|  4275527.0|2025|    8|\n",
      "+-------------------+------+------+------+------+-----------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify by reading back the parquet files\n",
    "df_parquet = spark.read.parquet(parquet_output)\n",
    "print(f\"Parquet verification - Rows: {df_parquet.count()}, Columns: {len(df_parquet.columns)}\")\n",
    "df_parquet.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "759b3d05-e019-4fc8-98d9-630ba7878f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technical Indicators Calculated: SMA_200, EMA_50, RSI_14, MACD, MACD_Signal, MACD_Histogram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+-------------------+-------------------+------------------+----+-----------+--------------+\n",
      "|datetime           |Close |SMA_200            |EMA_50             |RSI_14            |MACD|MACD_Signal|MACD_Histogram|\n",
      "+-------------------+------+-------------------+-------------------+------------------+----+-----------+--------------+\n",
      "|2023-01-01 01:00:00|0.0697|0.0697             |0.0697             |NULL              |NULL|NULL       |NULL          |\n",
      "|2023-01-01 05:00:00|0.0694|0.06955            |0.06968823529411763|NULL              |NULL|NULL       |NULL          |\n",
      "|2023-01-01 09:00:00|0.0696|0.06956666666666667|0.06968477508650518|NULL              |NULL|NULL       |NULL          |\n",
      "|2023-01-01 13:00:00|0.0699|0.06965            |0.06969321527919126|NULL              |NULL|NULL       |NULL          |\n",
      "|2023-01-01 17:00:00|0.0702|0.06976            |0.0697130891898112 |NULL              |NULL|NULL       |NULL          |\n",
      "|2023-01-01 21:00:00|0.0703|0.06985000000000001|0.06973610530001469|NULL              |NULL|NULL       |NULL          |\n",
      "|2023-01-02 01:00:00|0.0699|0.06985714285714287|0.06974253254315137|NULL              |NULL|NULL       |NULL          |\n",
      "|2023-01-02 05:00:00|0.0719|0.07011250000000001|0.06982713911008662|NULL              |NULL|NULL       |NULL          |\n",
      "|2023-01-02 09:00:00|0.0718|0.0703             |0.06990450620380871|NULL              |NULL|NULL       |NULL          |\n",
      "|2023-01-02 13:00:00|0.0724|0.07051            |0.07000236870562014|NULL              |NULL|NULL       |NULL          |\n",
      "|2023-01-02 17:00:00|0.0723|0.07067272727272728|0.07009247189363504|NULL              |NULL|NULL       |NULL          |\n",
      "|2023-01-02 21:00:00|0.0715|0.07074166666666667|0.07014766907427682|NULL              |NULL|NULL       |NULL          |\n",
      "|2023-01-03 01:00:00|0.0716|0.07080769230769232|0.07020462322822675|NULL              |NULL|NULL       |NULL          |\n",
      "|2023-01-03 05:00:00|0.0721|0.0709             |0.0702789517290806 |70.68965517241374 |NULL|NULL       |NULL          |\n",
      "|2023-01-03 09:00:00|0.0715|0.07094            |0.070326835974999  |63.60381861575172 |NULL|NULL       |NULL          |\n",
      "|2023-01-03 13:00:00|0.0709|0.0709375          |0.07034931299558729|57.40679370339691 |NULL|NULL       |NULL          |\n",
      "|2023-01-03 17:00:00|0.07  |0.07088235294117648|0.07033561444674073|49.600233472462435|NULL|NULL       |NULL          |\n",
      "|2023-01-03 21:00:00|0.0705|0.07086111111111111|0.07034206093902541|53.392221885358055|NULL|NULL       |NULL          |\n",
      "|2023-01-04 01:00:00|0.0728|0.07096315789473684|0.07043845070612247|66.04709787256388 |NULL|NULL       |NULL          |\n",
      "|2023-01-04 05:00:00|0.0725|0.07104            |0.0705192957764706 |63.62062453281327 |NULL|NULL       |NULL          |\n",
      "+-------------------+------+-------------------+-------------------+------------------+----+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType, StructType, StructField\n",
    "\n",
    "# --- Config ---\n",
    "parquet_input = \"/datos/gittba26/gittba02/DOGE/doge_silver\"\n",
    "parquet_output = \"/datos/gittba26/gittba02/DOGE/doge_gold\"\n",
    "\n",
    "df_parquet = spark.read.parquet(parquet_input)\n",
    "\n",
    "# --- Base window ---\n",
    "w_ord = Window.orderBy(\"datetime\")\n",
    "\n",
    "# --- SMA 200 ---\n",
    "w_sma200 = w_ord.rowsBetween(-199, 0)\n",
    "df_indicators = df_parquet.withColumn(\"SMA_200\", F.avg(\"Close\").over(w_sma200))\n",
    "\n",
    "# --- Precompute delta, gain, loss (used by RSI) ---\n",
    "df_indicators = (\n",
    "    df_indicators\n",
    "    .withColumn(\"_delta\", F.col(\"Close\") - F.lag(\"Close\", 1).over(w_ord))\n",
    "    .withColumn(\"_gain\", F.when(F.col(\"_delta\") > 0, F.col(\"_delta\")).otherwise(0.0))\n",
    "    .withColumn(\"_loss\", F.when(F.col(\"_delta\") < 0, -F.col(\"_delta\")).otherwise(0.0))\n",
    ")\n",
    "\n",
    "# --- Collect lists for EMA/RSI/MACD in a single pass per window size ---\n",
    "ema_span_50 = 50\n",
    "w_ema50 = w_ord.rowsBetween(-ema_span_50 * 4, 0)\n",
    "\n",
    "rsi_period = 14\n",
    "w_rsi = w_ord.rowsBetween(-rsi_period * 10, 0)\n",
    "\n",
    "w_macd = w_ord.rowsBetween(-200, 0)\n",
    "\n",
    "df_indicators = (\n",
    "    df_indicators\n",
    "    .withColumn(\"_close_list_ema\", F.collect_list(\"Close\").over(w_ema50))\n",
    "    .withColumn(\"_gain_list\", F.collect_list(\"_gain\").over(w_rsi))\n",
    "    .withColumn(\"_loss_list\", F.collect_list(\"_loss\").over(w_rsi))\n",
    "    .withColumn(\"_close_list_macd\", F.collect_list(\"Close\").over(w_macd))\n",
    ")\n",
    "\n",
    "# --- UDFs ---\n",
    "alpha_50 = 2.0 / (ema_span_50 + 1)\n",
    "alpha_rsi = 1.0 / rsi_period\n",
    "alpha_12 = 2.0 / 13.0\n",
    "alpha_26 = 2.0 / 27.0\n",
    "alpha_9 = 2.0 / 10.0\n",
    "\n",
    "@F.udf(DoubleType())\n",
    "def compute_ema(close_list, alpha):\n",
    "    if not close_list:\n",
    "        return None\n",
    "    ema = float(close_list[0])\n",
    "    for c in close_list[1:]:\n",
    "        ema = alpha * float(c) + (1.0 - alpha) * ema\n",
    "    return ema\n",
    "\n",
    "@F.udf(DoubleType())\n",
    "def compute_rsi(gain_list, loss_list, alpha):\n",
    "    if not gain_list or len(gain_list) < 14:\n",
    "        return None\n",
    "    avg_gain = sum(float(g) for g in gain_list[:14]) / 14.0\n",
    "    avg_loss = sum(float(l) for l in loss_list[:14]) / 14.0\n",
    "    for i in range(14, len(gain_list)):\n",
    "        avg_gain = alpha * float(gain_list[i]) + (1.0 - alpha) * avg_gain\n",
    "        avg_loss = alpha * float(loss_list[i]) + (1.0 - alpha) * avg_loss\n",
    "    if avg_loss == 0:\n",
    "        return 100.0\n",
    "    rs = avg_gain / avg_loss\n",
    "    return 100.0 - (100.0 / (1.0 + rs))\n",
    "\n",
    "macd_schema = StructType([\n",
    "    StructField(\"MACD\", DoubleType()),\n",
    "    StructField(\"MACD_Signal\", DoubleType()),\n",
    "    StructField(\"MACD_Histogram\", DoubleType()),\n",
    "])\n",
    "\n",
    "@F.udf(macd_schema)\n",
    "def compute_macd(close_list, a12, a26, a9):\n",
    "    if not close_list or len(close_list) < 26:\n",
    "        return None\n",
    "    ema12 = ema26 = float(close_list[0])\n",
    "    macd_vals = []\n",
    "    for c in close_list:\n",
    "        c = float(c)\n",
    "        ema12 = a12 * c + (1.0 - a12) * ema12\n",
    "        ema26 = a26 * c + (1.0 - a26) * ema26\n",
    "        macd_vals.append(ema12 - ema26)\n",
    "    signal = macd_vals[0]\n",
    "    for m in macd_vals:\n",
    "        signal = a9 * m + (1.0 - a9) * signal\n",
    "    macd = macd_vals[-1]\n",
    "    return (macd, signal, macd - signal)\n",
    "\n",
    "# --- Apply UDFs and extract MACD fields ---\n",
    "df_indicators = (\n",
    "    df_indicators\n",
    "    .withColumn(\"EMA_50\", compute_ema(F.col(\"_close_list_ema\"), F.lit(alpha_50)))\n",
    "    .withColumn(\"RSI_14\", compute_rsi(F.col(\"_gain_list\"), F.col(\"_loss_list\"), F.lit(alpha_rsi)))\n",
    "    .withColumn(\"_macd\", compute_macd(F.col(\"_close_list_macd\"), F.lit(alpha_12), F.lit(alpha_26), F.lit(alpha_9)))\n",
    "    .withColumn(\"MACD\", F.col(\"_macd.MACD\"))\n",
    "    .withColumn(\"MACD_Signal\", F.col(\"_macd.MACD_Signal\"))\n",
    "    .withColumn(\"MACD_Histogram\", F.col(\"_macd.MACD_Histogram\"))\n",
    ")\n",
    "\n",
    "# --- Clean up and write ---\n",
    "temp_cols = [\"_delta\", \"_gain\", \"_loss\", \"_close_list_ema\", \"_gain_list\", \"_loss_list\", \"_close_list_macd\", \"_macd\"]\n",
    "df_final = df_indicators.drop(*temp_cols).orderBy(\"datetime\")\n",
    "\n",
    "df_final.write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .partitionBy(\"year\", \"month\") \\\n",
    "  .parquet(parquet_output)\n",
    "\n",
    "print(\"Technical Indicators Calculated: SMA_200, EMA_50, RSI_14, MACD, MACD_Signal, MACD_Histogram\")\n",
    "df_final.select(\"datetime\", \"Close\", \"SMA_200\", \"EMA_50\", \"RSI_14\", \"MACD\", \"MACD_Signal\", \"MACD_Histogram\").show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1987b582-0893-4dae-ad99-9752e8b8c75d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda",
   "language": "python",
   "name": "anaconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
